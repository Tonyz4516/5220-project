# if convert failed, it is not numeric
col_num[i] = NA
}
}
col_num = col_num[!is.na(col_num)]
# convert factor
count = 1
drop_col = rep(NA, 200)
col_fac = which(! 1:ncol(voter_train) %in% col_num)
# View(voter_train[1:100, col_fac])
for (i in 1:length(col_fac)) {
voter_train[,col_fac[i]] = as.factor(voter_train[,col_fac[i]])
if (length(levels(voter_train[,col_fac[i]])) <= 1) {
drop_col[count] = col_fac[i]
count = count + 1
}
}
drop_col = drop_col[!is.na(drop_col)]
voter_train = voter_train[,-drop_col]
voter_train = voter_train[,-c(1:2)]
# we only care about 3 classes for results, so convert:
r = as.character(voter_train$presvote16post_2016)
r = ifelse(r %in% c("donald trump", "hillary clinton"), r, "others")
voter_train$presvote16post_2016 = as.factor(r)
# repeat the same for voter_test
voter_test = apply(voter_test, 2,
function(x) Hmisc::impute(x, 'random'))
voter_test = as.data.frame(voter_test, stringsAsFactors = F)
for (i in 1:length(col_num)) {
voter_test[,col_num[i]] =
as.numeric(voter_test[,col_num[i]])
}
for (i in 1:length(col_fac)) {
voter_test[,col_fac[i]] = as.factor(voter_test[,col_fac[i]])
}
voter_test = voter_test[,-drop_col]
voter_test = voter_test[,-c(1:2)]
r1 = as.character(voter_test$presvote16post_2016)
r1 = ifelse(r1 %in% c("donald trump", "hillary clinton"),
r1, "others")
voter_test$presvote16post_2016 = as.factor(r1)
getwd()
setwd("C:/Users/yukua/Documents/GitHub/5220-project")
source("http://bit.ly/sentiment_required_packages")
load_package("caret")
load("data_clean/step3.rda")
category = read.xlsx("data_clean/variable_identification.xlsx")
set.seed(12345)
orders = sample(1:8000,8000)
voter_train = voter[orders[1:6400],]
voter_test = voter[orders[6401:8000],]
#
load_package("Hmisc")
# impute with random value
voter_train = apply(voter_train, 2,
function(x) Hmisc::impute(x, 'random'))
voter_train = as.data.frame(voter_train, stringsAsFactors = F)
load_package(c("randomForest", "caret", "e1071"))
# ordinal and contious variables
col_num = which(category$category %in% c("1.0", "3.0"))
for (i in 1:length(col_num)) {
if (sum(is.na(as.numeric(voter_train[,col_num[i]]))) == 0) {
voter_train[,col_num[i]] =
as.numeric(voter_train[,col_num[i]])
} else {
# if convert failed, it is not numeric
col_num[i] = NA
}
}
col_num = col_num[!is.na(col_num)]
# convert factor
count = 1
drop_col = rep(NA, 200)
col_fac = which(! 1:ncol(voter_train) %in% col_num)
# View(voter_train[1:100, col_fac])
for (i in 1:length(col_fac)) {
voter_train[,col_fac[i]] = as.factor(voter_train[,col_fac[i]])
if (length(levels(voter_train[,col_fac[i]])) <= 1) {
drop_col[count] = col_fac[i]
count = count + 1
}
}
drop_col = drop_col[!is.na(drop_col)]
voter_train = voter_train[,-drop_col]
voter_train = voter_train[,-c(1:2)]
# we only care about 3 classes for results, so convert:
r = as.character(voter_train$presvote16post_2016)
r = ifelse(r %in% c("donald trump", "hillary clinton"), r, "others")
voter_train$presvote16post_2016 = as.factor(r)
# repeat the same for voter_test
voter_test = apply(voter_test, 2,
function(x) Hmisc::impute(x, 'random'))
voter_test = as.data.frame(voter_test, stringsAsFactors = F)
for (i in 1:length(col_num)) {
voter_test[,col_num[i]] =
as.numeric(voter_test[,col_num[i]])
}
for (i in 1:length(col_fac)) {
voter_test[,col_fac[i]] = as.factor(voter_test[,col_fac[i]])
}
voter_test = voter_test[,-drop_col]
voter_test = voter_test[,-c(1:2)]
r1 = as.character(voter_test$presvote16post_2016)
r1 = ifelse(r1 %in% c("donald trump", "hillary clinton"),
r1, "others")
voter_test$presvote16post_2016 = as.factor(r1)
trControl <- trainControl(method = "cv",
number = 5,
search = "grid",
verboseIter = TRUE)
# start = Sys.time()
# set.seed(12345)
# tuneGrid <- expand.grid(.mtry = c(seq(10,120,10)))
# rf_mtry <- train(presvote16post_2016~.,
#     data = voter_train,
#     method = "rf",
#     metric = "Accuracy",
#     tuneGrid = tuneGrid,
#     trControl = trControl,
#     importance = TRUE,
#     nodesize = 14,
#     ntree = 300)
# # measure time of train
# print(Sys.time() - start)
#
# print(rf_mtry)
#
#
# search for the best maxnode
start = Sys.time()
store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = 110) # best mtry
testmaxnode <- function(maxnode) {
set.seed(1234)
rf_maxnode <- train(presvote16post_2016~.,
data = voter_train,
method = "rf",
metric = "Accuracy",
tuneGrid = tuneGrid,
trControl = trControl,
importance = TRUE,
nodesize = 14,
maxnodes = maxnodes,
ntree = 300)
current_iteration <- toString(maxnodes)
store_maxnode[[current_iteration]] <- rf_maxnode
}
results_mtry <- resamples(store_maxnode)
sink("mcrm_results.txt")
print(Sys.time() - start)
summary(results_mtry)
sink()
# #
# #
# # search for best ntree
# #
# #
# start = Sys.time()
# store_maxtrees <- list()
# for (ntree in c(250, 300, 350, 400, 450, 500, 550, 600, 800, 1000, 2000)) {
#     set.seed(5678)
#     rf_maxtrees <- train(presvote16post_2016~.,
#         data = voter_train,
#         method = "rf",
#         metric = "Accuracy",
#         tuneGrid = tuneGrid,
#         trControl = trControl,
#         importance = TRUE,
#         nodesize = 14,
#         maxnodes = 15,
#         ntree = ntree)
#     key <- toString(ntree)
#     store_maxtrees[[key]] <- rf_maxtrees
# }
# results_tree <- resamples(store_maxtrees)
# print(Sys.time() - start)
# #
# #
# #
# summary(results_tree)
# #
# #
# # ### fit with best parameter
# #
# #
# fit_rf <- train(presvote16post_2016~.,
#                 data = voter_train,
#                 method = "rf",
#                 metric = "Accuracy",
#                 tuneGrid = tuneGrid,
#                 trControl = trControl,
#                 importance = TRUE,
#                 nodesize = 14,
#                 ntree = 800,
#                 maxnodes = 24)
# #
# #
# # ### Evaluate the model
# #
# #
# prediction <-predict(fit_rf, voter_test)
# confusionMatrix(prediction, voter_test$presvote16post_2016)
# #
# #
# # variable importance
# #
# #
# varImp(fit_rf)
# #
# #
# # ## problems and fix
# #
# # The model has good fit, however, the most important variables determined by the model are "Do you have favorable or unfavorable opinions regarding Trump?" It make sense why the it is the most important variable, but it is not helpful for us to understand links between people's political opinion and their votes.
# #
# # Thus, we decided to drop all variables directly involving ask do you favor Trump or Clinton, and refit the model.
# #
# #
# vars_involve_choose <- c("pp_repprim16_2016",
#                          "pp_demprim16_2016",
#                          "fav_trump_2016",
#                          "fav_hrc_2016",
#                          "Sanders_Trump_2016",
#                          "Clinton_Cruz_2016",
#                          "Sanders_Rubio_2016",
#                          "Clinton_Rubio_2016")
# col <- which(colnames(voter_train) %in% vars_involve_choose)
# voter_train1 <- voter_train[,-col]
# voter_test1 <- voter_test[,-col]
# #
# #
# # re-train
# #
# #
# start = Sys.time()
# fit_rf1 <- train(presvote16post_2016~.,
#                 data = voter_train1,
#                 method = "rf",
#                 metric = "Accuracy",
#                 tuneGrid = tuneGrid,
#                 trControl = trControl,
#                 importance = TRUE,
#                 nodesize = 14,
#                 ntree = 800,
#                 maxnodes = 24)
# print(Sys.time() - start)
# #
# #
# #
# prediction <- predict(fit_rf1, voter_test1)
# confusionMatrix(prediction, voter_test1$presvote16post_2016)
# #
# #
# #
# varImp(fit_rf1)
?mclapply
#source("http://bit.ly/sentiment_required_packages")
require(caret)
require(randomForest)
require(e1071)
require(openxlsx)
load("data_clean/step3.rda")
category = read.xlsx("data_clean/variable_identification.xlsx")
set.seed(12345)
orders = sample(1:8000,8000)
voter_train = voter[orders[1:6400],]
voter_test = voter[orders[6401:8000],]
#
require(Hmisc)
# impute with random value
voter_train = apply(voter_train, 2,
function(x) Hmisc::impute(x, 'random'))
voter_train = as.data.frame(voter_train, stringsAsFactors = F)
# ordinal and contious variables
col_num = which(category$category %in% c("1.0", "3.0"))
for (i in 1:length(col_num)) {
if (sum(is.na(as.numeric(voter_train[,col_num[i]]))) == 0) {
voter_train[,col_num[i]] =
as.numeric(voter_train[,col_num[i]])
} else {
# if convert failed, it is not numeric
col_num[i] = NA
}
}
col_num = col_num[!is.na(col_num)]
# convert factor
count = 1
drop_col = rep(NA, 200)
col_fac = which(! 1:ncol(voter_train) %in% col_num)
# View(voter_train[1:100, col_fac])
for (i in 1:length(col_fac)) {
voter_train[,col_fac[i]] = as.factor(voter_train[,col_fac[i]])
if (length(levels(voter_train[,col_fac[i]])) <= 1) {
drop_col[count] = col_fac[i]
count = count + 1
}
}
drop_col = drop_col[!is.na(drop_col)]
voter_train = voter_train[,-drop_col]
voter_train = voter_train[,-c(1:2)]
# we only care about 3 classes for results, so convert:
r = as.character(voter_train$presvote16post_2016)
r = ifelse(r %in% c("donald trump", "hillary clinton"), r, "others")
voter_train$presvote16post_2016 = as.factor(r)
# repeat the same for voter_test
voter_test = apply(voter_test, 2,
function(x) Hmisc::impute(x, 'random'))
voter_test = as.data.frame(voter_test, stringsAsFactors = F)
for (i in 1:length(col_num)) {
voter_test[,col_num[i]] =
as.numeric(voter_test[,col_num[i]])
}
for (i in 1:length(col_fac)) {
voter_test[,col_fac[i]] = as.factor(voter_test[,col_fac[i]])
}
voter_test = voter_test[,-drop_col]
voter_test = voter_test[,-c(1:2)]
r1 = as.character(voter_test$presvote16post_2016)
r1 = ifelse(r1 %in% c("donald trump", "hillary clinton"),
r1, "others")
voter_test$presvote16post_2016 = as.factor(r1)
?install.packages
trControl <- trainControl(method = "cv",
number = 5,
search = "grid",
verboseIter = TRUE)
# start = Sys.time()
# set.seed(12345)
# tuneGrid <- expand.grid(.mtry = c(seq(10,120,10)))
# rf_mtry <- train(presvote16post_2016~.,
#     data = voter_train,
#     method = "rf",
#     metric = "Accuracy",
#     tuneGrid = tuneGrid,
#     trControl = trControl,
#     importance = TRUE,
#     nodesize = 14,
#     ntree = 300)
# # measure time of train
# print(Sys.time() - start)
#
# print(rf_mtry)
#
#
# search for the best maxnode
start = Sys.time()
store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = 110) # best mtry
testmaxnode <- function(maxnode) {
set.seed(1234)
rf_maxnode <- train(presvote16post_2016~.,
data = voter_train,
method = "rf",
metric = "Accuracy",
tuneGrid = tuneGrid,
trControl = trControl,
importance = TRUE,
nodesize = 14,
maxnodes = maxnodes,
ntree = 300)
# current_iteration <- toString(maxnodes)
return(rf_maxnode)
}
store_maxnode <- lapply(5:15, testmaxnode, mc.cores = 8)
results_mtry <- resamples(store_maxnode)
sink("mcrm_results.txt")
print(Sys.time() - start)
summary(results_mtry)
sink()
# #
# #
# # search for best ntree
# #
# #
# start = Sys.time()
# store_maxtrees <- list()
# for (ntree in c(250, 300, 350, 400, 450, 500, 550, 600, 800, 1000, 2000)) {
#     set.seed(5678)
#     rf_maxtrees <- train(presvote16post_2016~.,
#         data = voter_train,
#         method = "rf",
#         metric = "Accuracy",
#         tuneGrid = tuneGrid,
#         trControl = trControl,
#         importance = TRUE,
#         nodesize = 14,
#         maxnodes = 15,
#         ntree = ntree)
#     key <- toString(ntree)
#     store_maxtrees[[key]] <- rf_maxtrees
# }
# results_tree <- resamples(store_maxtrees)
# print(Sys.time() - start)
# #
# #
# #
# summary(results_tree)
# #
# #
# # ### fit with best parameter
# #
# #
# fit_rf <- train(presvote16post_2016~.,
#                 data = voter_train,
#                 method = "rf",
#                 metric = "Accuracy",
#                 tuneGrid = tuneGrid,
#                 trControl = trControl,
#                 importance = TRUE,
#                 nodesize = 14,
#                 ntree = 800,
#                 maxnodes = 24)
# #
# #
# # ### Evaluate the model
# #
# #
# prediction <-predict(fit_rf, voter_test)
# confusionMatrix(prediction, voter_test$presvote16post_2016)
# #
# #
# # variable importance
# #
# #
# varImp(fit_rf)
# #
# #
# # ## problems and fix
# #
# # The model has good fit, however, the most important variables determined by the model are "Do you have favorable or unfavorable opinions regarding Trump?" It make sense why the it is the most important variable, but it is not helpful for us to understand links between people's political opinion and their votes.
# #
# # Thus, we decided to drop all variables directly involving ask do you favor Trump or Clinton, and refit the model.
# #
# #
# vars_involve_choose <- c("pp_repprim16_2016",
#                          "pp_demprim16_2016",
#                          "fav_trump_2016",
#                          "fav_hrc_2016",
#                          "Sanders_Trump_2016",
#                          "Clinton_Cruz_2016",
#                          "Sanders_Rubio_2016",
#                          "Clinton_Rubio_2016")
# col <- which(colnames(voter_train) %in% vars_involve_choose)
# voter_train1 <- voter_train[,-col]
# voter_test1 <- voter_test[,-col]
# #
# #
# # re-train
# #
# #
# start = Sys.time()
# fit_rf1 <- train(presvote16post_2016~.,
#                 data = voter_train1,
#                 method = "rf",
#                 metric = "Accuracy",
#                 tuneGrid = tuneGrid,
#                 trControl = trControl,
#                 importance = TRUE,
#                 nodesize = 14,
#                 ntree = 800,
#                 maxnodes = 24)
# print(Sys.time() - start)
# #
# #
# #
# prediction <- predict(fit_rf1, voter_test1)
# confusionMatrix(prediction, voter_test1$presvote16post_2016)
# #
# #
# #
# varImp(fit_rf1)
trControl <- trainControl(method = "cv",
number = 5,
search = "grid",
verboseIter = TRUE)
start = Sys.time()
store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = 110) # best mtry
testmaxnode <- function(maxnode) {
set.seed(1234)
rf_maxnode <- train(presvote16post_2016~.,
data = voter_train,
method = "rf",
metric = "Accuracy",
tuneGrid = tuneGrid,
trControl = trControl,
importance = TRUE,
nodesize = 14,
maxnodes = maxnodes,
ntree = 300)
# current_iteration <- toString(maxnodes)
return(rf_maxnode)
}
store_maxnode <- lapply(5:15, testmaxnode) #, mc.cores = 8
start = Sys.time()
store_maxnode <- list()
tuneGrid <- expand.grid(.mtry = 110) # best mtry
testmaxnode <- function(maxnodes) {
set.seed(1234)
rf_maxnode <- train(presvote16post_2016~.,
data = voter_train,
method = "rf",
metric = "Accuracy",
tuneGrid = tuneGrid,
trControl = trControl,
importance = TRUE,
nodesize = 14,
maxnodes = maxnodes,
ntree = 300)
# current_iteration <- toString(maxnodes)
return(rf_maxnode)
}
store_maxnode <- lapply(5:15, testmaxnode) #, mc.cores = 8
